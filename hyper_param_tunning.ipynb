{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c016345f",
   "metadata": {},
   "source": [
    "# ECBM 4040 Fall '21 Project  - BIOM Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cf1a6",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from model.models_cstm import get_embedding_model\n",
    "from model.train_model import train_siamese_model\n",
    "\n",
    "!pip3 install keras-tuner\n",
    "import keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec42589",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_METRICS = {\n",
    "    'EUCLIDEAN': 'euclidean',\n",
    "    'HYPERBOLIC': 'hyperbolic',\n",
    "    'MANHATTAN': 'manhattan',\n",
    "    'SQUARE': 'square',\n",
    "    'COSINE': 'cosine'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seeds for reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad4aae",
   "metadata": {},
   "source": [
    "## Get Qiita Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/mv546rx259tgwaz/qiita_numpy.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "shutil.move(f\"{cwd}/qiita_numpy.pkl\", f\"{cwd}/data/qiita/qiita_numpy.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24c59e",
   "metadata": {},
   "source": [
    "## Load Qiita Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QIITA dataset.\n",
    "((X_train, X_test, X_val), (y_train, y_test, y_val)) = pickle.load(open(f\"{cwd}/data/qiita/qiita_numpy.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92e9df",
   "metadata": {},
   "source": [
    "## Distance Function Hyperparam Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_embedding_model()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Siamese Model\n",
    "\n",
    "data = ((X_train, X_test, X_val), (y_train, y_test, y_val))\n",
    "\n",
    "dist_func_tunning = {}\n",
    "for key in DISTANCE_METRICS.keys():\n",
    "    dist = DISTANCE_METRICS[key]\n",
    "    _, score, history = train_siamese_model(data, embedding, dist , batch_size=256, epochs = 1)\n",
    "    dist_func_tunning[key] = {'score': score, 'history': history.history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = open(\"results/dense/dist_func_tunning.pkl\", \"wb\")\n",
    "pickle.dump(dist_func_tunning, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd74c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f630a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aafa6c4",
   "metadata": {},
   "source": [
    "## Visualize Dist_Func_Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fa695",
   "metadata": {},
   "source": [
    "# HyperParam Tunning with RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e26af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import DistanceLayer\n",
    "from model.models_cstm import SiameseModel\n",
    "from model.generator import SequenceDistDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d8c45",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ Hyperbolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86775cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    embedding_model = get_embedding_model()\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['HYPERBOLIC'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 512, step=32)\n",
    "        kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 30, step = 2)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder, objective='val_loss' )\n",
    "\n",
    "training_generator = SequenceDistDataGenerator( X_train, y_train )\n",
    "validation_generator = SequenceDistDataGenerator( X_val, y_val)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(training_generator, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_HYPERBOLIC=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_HYPERBOLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0efae",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ EUCLIDEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b092e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    embedding_model = get_embedding_model()\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['EUCLIDEAN'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 512, step=32)\n",
    "        kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 30, step = 2)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder, objective='val_loss' )\n",
    "\n",
    "training_generator = SequenceDistDataGenerator( X_train, y_train )\n",
    "validation_generator = SequenceDistDataGenerator( X_val, y_val)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(training_generator, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_EUCLIDEAN=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_EUCLIDEAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
