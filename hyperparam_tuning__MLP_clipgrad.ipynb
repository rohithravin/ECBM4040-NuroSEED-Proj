{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c016345f",
   "metadata": {},
   "source": [
    "# ECBM 4040 Fall '21 Project  - BIOM Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9208427",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: NeuroSEED MLP Model (with gradient clipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd1825",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09d5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /home/ecbm4040/envTF24/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: requests in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (2.26.0)\n",
      "Requirement already satisfied: tensorboard in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (2.6.0)\n",
      "Requirement already satisfied: numpy in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (21.0)\n",
      "Requirement already satisfied: ipython in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (7.16.1)\n",
      "Requirement already satisfied: scipy in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (1.5.4)\n",
      "Requirement already satisfied: kt-legacy in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: pexpect in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: pygments in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (2.10.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (4.3.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (0.18.0)\n",
      "Requirement already satisfied: decorator in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (5.0.9)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (3.0.20)\n",
      "Requirement already satisfied: backcall in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from ipython->keras-tuner) (58.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: six in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from traitlets>=4.2->ipython->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests->keras-tuner) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests->keras-tuner) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (3.17.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (2.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (1.35.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from tensorboard->keras-tuner) (1.32.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
      "Requirement already satisfied: dataclasses in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard->keras-tuner) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ecbm4040/envTF24/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from model.models_cstm import get_embedding_model\n",
    "from model.train_model import train_siamese_model\n",
    "\n",
    "!pip3 install keras-tuner\n",
    "import keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec42589",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_METRICS = {\n",
    "    'EUCLIDEAN': 'euclidean',\n",
    "    'HYPERBOLIC': 'hyperbolic',\n",
    "    'MANHATTAN': 'manhattan',\n",
    "    'SQUARE': 'square',\n",
    "    'COSINE': 'cosine'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667d55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seeds for reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad4aae",
   "metadata": {},
   "source": [
    "## Get Qiita Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f6cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "if not os.path.exists(f\"{cwd}/data/qiita/qiita_numpy.pkl\"):\n",
    "    !wget https://www.dropbox.com/s/mv546rx259tgwaz/qiita_numpy.pkl\n",
    "    shutil.move(f\"{cwd}/qiita_numpy.pkl\", f\"{cwd}/data/qiita/qiita_numpy.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24c59e",
   "metadata": {},
   "source": [
    "## Load Qiita Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbb0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QIITA dataset.\n",
    "((X_train, X_test, X_val), (y_train, y_test, y_val)) = pickle.load(open(f\"{cwd}/data/qiita/qiita_numpy.pkl\", \"rb\"))\n",
    "\n",
    "dat_lim = len(X_train)\n",
    "data = ((X_train[:dat_lim], X_test[:dat_lim], X_val[:dat_lim]), (y_train[:dat_lim,:dat_lim], y_test[:dat_lim], y_val[:dat_lim,:dat_lim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92e9df",
   "metadata": {},
   "source": [
    "## Distance Function Hyperparam Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc5c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "one_hot_encoding_layer (OneH (None, 152, 4)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 608)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 3045      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               768       \n",
      "=================================================================\n",
      "Total params: 3,843\n",
      "Trainable params: 3,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding_model(model_choice='MLP')\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3b768a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47844/47844 [==============================] - 756s 16ms/step - loss: 1021.2123 - val_loss: 759.1774\n",
      "Epoch 2/5\n",
      "47844/47844 [==============================] - 760s 16ms/step - loss: 725.1674 - val_loss: 749.7034\n",
      "Epoch 3/5\n",
      "47844/47844 [==============================] - 750s 16ms/step - loss: 701.9582 - val_loss: 752.1136\n",
      "Epoch 4/5\n",
      "47844/47844 [==============================] - 706s 15ms/step - loss: 690.9645 - val_loss: 752.2046\n",
      "Epoch 5/5\n",
      "47844/47844 [==============================] - 637s 13ms/step - loss: 684.1503 - val_loss: 749.2722\n",
      "2195/2195 [==============================] - 14s 7ms/step - loss: 775.5123\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5f535af1946c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_EUCLIDEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_siamese_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISTANCE_METRICS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EUCLIDEAN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdist_func_tunning_EUCLIDEAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfile_to_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdist_res_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"dist_func_tunning_EUCLIDEAN.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_func_tunning_EUCLIDEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_to_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "dist_res_path = \"results/mlp_clipgrad/\"\n",
    "\n",
    "model_EUCLIDEAN, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['EUCLIDEAN'] , batch_size=512, epochs = 5)\n",
    "dist_func_tunning_EUCLIDEAN = {'score': score, 'history': history.history, 'model':model_EUCLIDEAN}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_EUCLIDEAN.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_EUCLIDEAN, file_to_write)\n",
    "\n",
    "model_HYPERBOLIC, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['HYPERBOLIC'] , batch_size=512, epochs = 5)\n",
    "dist_func_tunning_HYPERBOLIC = {'score': score, 'history': history.history, 'model':model_HYPERBOLIC}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_HYPERBOLIC.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_HYPERBOLIC, file_to_write)\n",
    "\n",
    "model_MANHATTAN, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['MANHATTAN'] , batch_size=512, epochs = 5)\n",
    "dist_func_tunning_MANHATTAN = {'score': score, 'history': history.history, 'model':model_MANHATTAN}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_MANHATTAN.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_MANHATTAN, file_to_write)\n",
    "\n",
    "model_SQUARE, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['SQUARE'] , batch_size=512, epochs = 5)\n",
    "dist_func_tunning_SQUARE = {'score': score, 'history': history.history, 'model':model_SQUARE}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_SQUARE.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_SQUARE, file_to_write)\n",
    "\n",
    "model_COSINE, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['COSINE'] , batch_size=512, epochs = 5)\n",
    "dist_func_tunning_COSINE = {'score': score, 'history': history.history, 'model':model_COSINE}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_COSINE.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_COSINE, file_to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb1dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd74c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f630a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aafa6c4",
   "metadata": {},
   "source": [
    "## Visualize Dist_Func_Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fa695",
   "metadata": {},
   "source": [
    "# HyperParam Tunning with RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e26af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import DistanceLayer\n",
    "from model.models_cstm import SiameseModel\n",
    "from model.generator import SequenceDistDataGenerator\n",
    "\n",
    "def model_builder_hyp(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    embedding_model = get_embedding_model(model_choice='MLP')\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['HYPERBOLIC'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate, clipnorm=1)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_builder_euc(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    embedding_model = get_embedding_model(model_choice='MLP')\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['EUCLIDEAN'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate, clipnorm=1)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 128, 512, step=128)\n",
    "        kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 15, step = 2)\n",
    "\n",
    "        X_val, y_val = kwargs['validation_data']\n",
    "        training_generator = SequenceDistDataGenerator( X_train, y_train, batch_size = kwargs['batch_size'] )\n",
    "        kwargs['validation_data'] = SequenceDistDataGenerator( X_val, y_val, batch_size = kwargs['batch_size'])\n",
    "        \n",
    "        return super(MyTuner, self).run_trial(trial, training_generator, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d8c45",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ Hyperbolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86775cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder_hyp, objective='val_loss' , directory='hyper_param_tunning',\n",
    "                     project_name='random_search_cv_hyp_mlp_clipgrad', max_trials=10)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(X_train, y_train,  validation_data=(X_val,y_val),\n",
    "             callbacks= [tf.keras.callbacks.EarlyStopping('val_loss', patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_HYPERBOLIC=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_HYPERBOLIC\n",
    "file_to_write = open( dist_rest_path + \"best_hyps_HYPERBOLIC.pkl\", \"wb\")\n",
    "pickle.dump(best_hps_HYPERBOLIC, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0efae",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ EUCLIDEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b092e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder_euc, objective='val_loss' directory='hyper_param_tunning',\n",
    "                     project_name='random_search_cv_euc_mlp_clipgrad',  max_trials=10)\n",
    "\n",
    "training_generator = SequenceDistDataGenerator( X_train, y_train )\n",
    "validation_generator = SequenceDistDataGenerator( X_val, y_val)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(training_generator, validation_data=validation_generator,\n",
    "            max_trials=5, callbacks= [tf.keras.callbacks.EarlyStopping('val_loss', patience=2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_EUCLIDEAN=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_EUCLIDEAN\n",
    "file_to_write = open( dist_res_path + \"best_hyps_EUCLIDEAN.pkl\", \"wb\")\n",
    "pickle.dump(best_hps_EUCLIDEAN, file_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
