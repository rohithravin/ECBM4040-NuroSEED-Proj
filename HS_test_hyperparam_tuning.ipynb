{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c016345f",
   "metadata": {},
   "source": [
    "# ECBM 4040 Fall '21 Project  - BIOM Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7875f48",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: NeuroSEED Linear Model (with gradient clipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd1825",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09d5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Program Files\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (1.1.0)\n",
      "Requirement already satisfied: ipython in c:\\program files\\anaconda\\lib\\site-packages (from keras-tuner) (7.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (1.21.3)\n",
      "Requirement already satisfied: packaging in c:\\program files\\anaconda\\lib\\site-packages (from keras-tuner) (20.9)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (2.7.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (1.5.2)\n",
      "Requirement already satisfied: requests in c:\\program files\\anaconda\\lib\\site-packages (from keras-tuner) (2.25.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (5.0.5)\n",
      "Requirement already satisfied: pygments in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (52.0.0.post20210125)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (3.0.17)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (0.17.2)\n",
      "Requirement already satisfied: backcall in c:\\program files\\anaconda\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\program files\\anaconda\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\program files\\anaconda\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\program files\\anaconda\\lib\\site-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\program files\\anaconda\\lib\\site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\anaconda\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\anaconda\\lib\\site-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\program files\\anaconda\\lib\\site-packages (from requests->keras-tuner) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from requests->keras-tuner) (1.25.7)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (3.18.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.41.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (0.14.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner) (0.36.2)\n",
      "Requirement already satisfied: six in c:\\program files\\anaconda\\lib\\site-packages (from absl-py>=0.4->tensorboard->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hannahsteele\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from model.models_cstm import get_embedding_model\n",
    "from model.train_model import train_siamese_model\n",
    "\n",
    "!pip3 install keras-tuner\n",
    "import keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec42589",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_METRICS = {\n",
    "    'EUCLIDEAN': 'euclidean',\n",
    "    'HYPERBOLIC': 'hyperbolic',\n",
    "    'MANHATTAN': 'manhattan',\n",
    "    'SQUARE': 'square',\n",
    "    'COSINE': 'cosine'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667d55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seeds for reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccd37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set static parameters\n",
    "MODEL_CHOICE = 'LINEAR'\n",
    "DEFAULT_BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_DAT_LIM = np.nan\n",
    "\n",
    "# set tuning parameters to search over\n",
    "LR_SELECTIONS = [1e-2, 1e-3, 1e-4]\n",
    "DROPOUT_SELECTIONS = [0.3, 0.5, 0.7]\n",
    "ACTIVATION_SELECTIONS = ['relu','relu','tanh']\n",
    "NUM_UNITS_SELECTIONS = [128, 256, 512] # NOTE: Not used for linear model\n",
    "NUM_FILTERS_SELECTIONS = [2,3,4] # NOTE: Not used for linear model\n",
    "#raise('PLACEHOLDER FOR BATCH SIZE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "dist_res_path = \"results/{0}_clipgrad/\".format(MODEL_CHOICE.lower())\n",
    "os.mkdir(dist_res_path)\n",
    "hyp_tuner_project_name=\"random_search_hyp_{0}_clipgrad\".format(MODEL_CHOICE.lower())\n",
    "euc_tuner_project_name=\"random_search_euc_{0}_clipgrad\".format(MODEL_CHOICE.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad4aae",
   "metadata": {},
   "source": [
    "## Get Qiita Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f6cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "if not os.path.exists(f\"{cwd}/data/qiita/qiita_numpy.pkl\"):\n",
    "    !wget https://www.dropbox.com/s/mv546rx259tgwaz/qiita_numpy.pkl\n",
    "    shutil.move(f\"{cwd}/qiita_numpy.pkl\", f\"{cwd}/data/qiita/qiita_numpy.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24c59e",
   "metadata": {},
   "source": [
    "## Load Qiita Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbb0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e89efd3024dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdat_lim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdat_lim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Load QIITA dataset.\n",
    "((X_train, X_test, X_val), (y_train, y_test, y_val)) = pickle.load(open(f\"{cwd}/data/qiita/qiita_numpy.pkl\", \"rb\"))\n",
    "print('X_train sample:\\n{0}'.format(X_train))\n",
    "dat_lim = TRAIN_DAT_LIM if not np.isnan(TRAIN_DAT_LIM) else len(X_train)\n",
    "data = ((X_train[:dat_lim], X_test[:dat_lim], X_val[:dat_lim]), (y_train[:dat_lim,:dat_lim], y_test[:dat_lim], y_val[:dat_lim,:dat_lim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92e9df",
   "metadata": {},
   "source": [
    "## Distance Function Hyperparam Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_embedding_model(model_choice=MODEL_CHOICE)\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EUCLIDEAN, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['EUCLIDEAN'], batch_size=DEFAULT_BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "dist_func_tunning_EUCLIDEAN = {'score': score, 'history': history.history, 'model':model_EUCLIDEAN}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_EUCLIDEAN.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_EUCLIDEAN, file_to_write)\n",
    "\n",
    "model_HYPERBOLIC, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['HYPERBOLIC'], batch_size=DEFAULT_BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "dist_func_tunning_HYPERBOLIC = {'score': score, 'history': history.history, 'model':model_HYPERBOLIC}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_HYPERBOLIC.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_HYPERBOLIC, file_to_write)\n",
    "\n",
    "model_MANHATTAN, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['MANHATTAN'], batch_size=DEFAULT_BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "dist_func_tunning_MANHATTAN = {'score': score, 'history': history.history, 'model':model_MANHATTAN}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_MANHATTAN.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_MANHATTAN, file_to_write)\n",
    "\n",
    "model_SQUARE, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['SQUARE'], batch_size=DEFAULT_BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "dist_func_tunning_SQUARE = {'score': score, 'history': history.history, 'model':model_SQUARE}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_SQUARE.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_SQUARE, file_to_write)\n",
    "\n",
    "model_COSINE, score, history = train_siamese_model(data, embedding, DISTANCE_METRICS['COSINE'], batch_size=DEFAULT_BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "dist_func_tunning_COSINE = {'score': score, 'history': history.history, 'model':model_COSINE}\n",
    "file_to_write = open( dist_res_path + \"dist_func_tunning_COSINE.pkl\", \"wb\" )\n",
    "pickle.dump(dist_func_tunning_COSINE, file_to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafa6c4",
   "metadata": {},
   "source": [
    "## Visualize Dist_Func_Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fa695",
   "metadata": {},
   "source": [
    "# HyperParam Tunning with RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e26af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import DistanceLayer\n",
    "from model.models_cstm import SiameseModel\n",
    "from model.generator import SequenceDistDataGenerator\n",
    "\n",
    "def model_builder_hyp(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    hp_act_func = hp.Choice('act_func', values=ACTIVATION_SELECTIONS)\n",
    "    hp_dropout = hp.Choice('dropout', values=DROPOUT_SELECTIONS)\n",
    "    hp_num_units = hp.Choice('mlp_num_units_hidden', values=NUM_UNITS_SELECTIONS)\n",
    "    hp_num_filters = hp.Choice('cnn_num_filters', values=NUM_FILTERS_SELECTIONS)\n",
    "    \n",
    "    embedding_model = get_embedding_model(model_choice=MODEL_CHOICE, act_func=hp_act_func, dropout=hp_dropout, \n",
    "                                         mlp_num_units_hidden=hp_num_units, cnn_num_filters=hp_num_filters)\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['HYPERBOLIC'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=LR_SELECTIONS)\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate, clipnorm=1)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_builder_euc(hp):\n",
    "    # Model definitions\n",
    "    in1 = tf.keras.layers.Input(name=\"sequence1\", shape=(152,))\n",
    "    in2 = tf.keras.layers.Input(name=\"sequence2\", shape=(152,))\n",
    "    \n",
    "    hp_act_func = hp.Choice('act_func', values=ACTIVATION_SELECTIONS)\n",
    "    hp_dropout = hp.Choice('dropout', values=DROPOUT_SELECTIONS)\n",
    "    hp_num_units = hp.Choice('mlp_num_units_hidden', values=NUM_UNITS_SELECTIONS)\n",
    "    hp_num_filters = hp.Choice('cnn_num_filters', values=NUM_FILTERS_SELECTIONS)\n",
    "    \n",
    "    embedding_model = get_embedding_model(model_choice=MODEL_CHOICE, act_func=hp_act_func, dropout=hp_dropout, \n",
    "                                         mlp_num_units_hidden=hp_num_units, cnn_num_filters=hp_num_filters)\n",
    "    \n",
    "    distance = DistanceLayer(metric=DISTANCE_METRICS['EUCLIDEAN'], dynamic=True)(\n",
    "        embedding_model(in1), \n",
    "        embedding_model(in2)\n",
    "    )\n",
    "\n",
    "    siamese_network = tf.keras.models.Model(\n",
    "        inputs=[in1, in2],\n",
    "        outputs=distance\n",
    "    )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=LR_SELECTIONS)\n",
    "    optimizer  = tf.keras.optimizers.Adam(hp_learning_rate, clipnorm=1)\n",
    "    \n",
    "    model = SiameseModel(siamese_network) # Depends on SiameseModel class, which we can define elsewhere\n",
    "    model.compile(optimizer=optimizer) # run_eagerly is not necessary, but useful for debugging\n",
    "    \n",
    "    return model\n",
    "\n",
    "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 128, 512, step=128)\n",
    "        kwargs['epochs'] = NUM_EPOCHS\n",
    "\n",
    "        X_val, y_val = kwargs['validation_data']\n",
    "        training_generator = SequenceDistDataGenerator( X_train, y_train, batch_size = kwargs['batch_size'] )\n",
    "        kwargs['validation_data'] = SequenceDistDataGenerator( X_val, y_val, batch_size = kwargs['batch_size'])\n",
    "        \n",
    "        return super(MyTuner, self).run_trial(trial, training_generator, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d8c45",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ Hyperbolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86775cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder_hyp, objective='val_loss' , directory='hyper_param_tunning',\n",
    "                     project_name=hyp_tuner_project_name, max_trials=10)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(X_train, y_train,  validation_data=(X_val,y_val),\n",
    "             callbacks= [tf.keras.callbacks.EarlyStopping('val_loss', patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_HYPERBOLIC=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_HYPERBOLIC\n",
    "file_to_write = open( dist_rest_path + \"best_hyps_HYPERBOLIC.pkl\", \"wb\")\n",
    "pickle.dump(best_hps_HYPERBOLIC, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0efae",
   "metadata": {},
   "source": [
    "## RandomSearchCV Hyperparam Tuning (w/ Euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b092e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(model_builder_euc, objective='val_loss' directory='hyper_param_tunning',\n",
    "                     project_name=euc_tuner_project_name,  max_trials=10)\n",
    "\n",
    "training_generator = SequenceDistDataGenerator( X_train, y_train )\n",
    "validation_generator = SequenceDistDataGenerator( X_val, y_val)\n",
    "\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(training_generator, validation_data=validation_generator,\n",
    "            max_trials=5, callbacks= [tf.keras.callbacks.EarlyStopping('val_loss', patience=2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps_EUCLIDEAN=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_EUCLIDEAN\n",
    "file_to_write = open( dist_res_path + \"best_hyps_EUCLIDEAN.pkl\", \"wb\")\n",
    "pickle.dump(best_hps_EUCLIDEAN, file_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
